---
title: "Project Vault"
author: "Andre Contreras"
date: "2023-2024"
output:   
  prettydoc::html_pretty:
    theme: tactile
params:
  num_beds:
    label: "Number of Bedrooms"
    input: text
    value: 3
  num_baths:
    label: "Number of Bathrooms"
    input: text
    value: 3
  living_space:
    label: "Living Space Area (sqft)"
    input: text
    value: 3000
  state:
    label: "State of Residence"
    input: text
    value: "Texas"
  circuitId_isWinner:
    label: "Enter the track's circuitId"
    input: text
    value: 7
  year_isWinner:
    label: "Enter any year after 2021"
    input: text
    value: 2024 
---

#### This page covers a diverse range of data analytics, data science, and machine learning techniques and topics I have worked on, each focusing on a different event or domain. Hope you find my findings interesting!

**The code I used for the following projects can all be found in my GitHub Repository:** ***https://github.com/andrejcc04/Portfolio***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# General
library(ggplot2) # used to plot
library(dplyr) # Used to manipulate, clean, filter data
library(broom)

# For F1 SVM/GB/RF Models
library(randomForest) # Machine Learning RF Model (Classification & Regression)
library(e1071) # Machine Learning SVM Model (Classification)
library(gbm) # Machine Learning GB Model (Classification)

# For ML XLK
library(caret) # Common for Machine Learning Techniques
library(readr) # Used to import dataset into R
library(tidyverse)

# For ML House Market
library(datasets) # for state abbreviation in ggplot

# For NFL:
library(readxl) # Used to read excel files imported into R
library(ggrepel) # Used for plot labeling

# For Olympics:
library(gapminder)
library(countrycode)
```


# **Machine Learning Models**

## 1. Ensemble Methods Model to predict **Race Winner**

  As I was watching the 2024 Monaco Grand Prix- I wondered how hard it would be to build a Machine Learning model that could (somewhat) successfully predict the winner of the races to come, so I got to work and after a couple of days of trial and error I managed to build a model. 
  I started off with just a Random Forest Classification model, but it wasn't giving me the results I wanted. Therefore, I figured I should create 3 different classification models (Support Vector Machine, Random Forest, and Gradient Boosting) to then combine them all in this Ensemble Methods model and see if that got me more precise results... It did!

**P.S I was also able to build a model to predict X driver's lap time on lap Y in the circuit Z, but that html output alone is bigger than this one so I might just keep it in a separate html:** ***https://rpubs.com/andrejcc/1192931***

```{r, echo=FALSE}
races <- read.csv("/Users/ajcon/Downloads/Portfolio/F1 Race Data/races.csv", na.strings = "\\N")
lap_times <- read.csv("/Users/ajcon/Downloads/Portfolio/F1 Race Data/lap_times.csv", na.strings = "\\N")
results <- read.csv("/Users/ajcon/Downloads/Portfolio/F1 Race Data/results.csv", na.strings = "\\N")
drivers <- read.csv("/Users/ajcon/Downloads/Portfolio/F1 Race Data/drivers.csv", na.strings = "\\N")
circuits <- read.csv("/Users/ajcon/Downloads/Portfolio/F1 Race Data/circuits.csv", na.strings = "\\N")
driver_standings <- read.csv("/Users/ajcon/Downloads/Portfolio/F1 Race Data/driver_standings.csv", na.strings = "\\N")
qualifying <- read.csv("/Users/ajcon/Downloads/Portfolio/F1 Race Data/qualifying.csv", na.strings = "\\N")
status <- read.csv("/Users/ajcon/Downloads/Portfolio/F1 Race Data/status.csv", na.strings = "\\N")


# Step 1: Extract Features
f1data <- left_join(races, circuits, by = "circuitId")
f1data <- left_join(f1data, results, by = "raceId")
f1data <- left_join(f1data, drivers, by = "driverId")
f1data <- left_join(f1data, status, by = "statusId")

f1data <- na.omit(f1data) %>% 
  filter(year >= 2021) %>%
  select(circuitId, constructorId, year, driverId, grid, position, milliseconds) %>%
  mutate(isWinner = ifelse(position == 1, TRUE, FALSE)) %>% # Feature Engineering
  mutate(grid = ifelse(grid == 0, 21, grid)) %>%
  select(-position)

set.seed(123)

f1data$isWinner <- as.factor(f1data$isWinner) # only 2 levels -- true and false so classification

# STEP 2: SPLIT DATA INTO TRAIN AND TEST SETS
split <- createDataPartition(f1data$isWinner, p = 0.8, list = FALSE)
train_data <- f1data[split, ]
test_data <- f1data[-split, ]



# --------------------------- SUPPORT VECTOR MACHINE ---------------------------
# The SVM is a supervised learning machine that classifies data by finding an optimal line that maximizes the distance between each class in an N-dimensional space.

# STEP 3A: TRAIN SVM MODEL (Classification)
svm_model <- svm(isWinner ~  constructorId + driverId + grid + year + circuitId, data = train_data, kernel = "poly", gamma = 0.25, cost = 2000, degree = 3) # Hyperparameter tuning (kernel, gamma, cost, degree)

svm_predictions <- predict(svm_model, newdata = test_data)

svm_binary_predictions <- ifelse(svm_predictions == TRUE, 1, 0)

# -------------------------- RANDOM FOREST CLASSIFIER --------------------------
# The Random Forest model grows multiple decision trees which are merged together for a more accurate prediction. The logic behind the Random Forest model is that multiple uncorrelated models (the individual decision trees) perform much better as a group than they do alone.

# STEP 3B: TRAIN RF MODEL (Classification)

rf_model <- randomForest(isWinner ~ constructorId + driverId + grid + year + circuitId, data = train_data, ntree = 500, mtry = 1, nodesize = 20)

rf_predictions <- predict(rf_model, newdata = test_data, type = "response")

rf_binary_predictions <- as.numeric(rf_predictions) - 1


# ------------------------ GRADIENT BOOSITNG CLASSIFIER ------------------------
# Boosting is one kind of ensemble Learning method which trains the model sequentially and each new model tries to correct the previous model. It combines several weak learners into strong learners.

# STEP 3C: TRAIN GBM MODEL (Classification)

train_data$isWinner_binary <- as.numeric(train_data$isWinner) - 1

train_data <- train_data %>% 
  select(circuitId, constructorId, year, driverId, grid, milliseconds, isWinner_binary)

gb_model <- gbm(isWinner_binary ~ constructorId + driverId + grid + year + circuitId, data = train_data, distribution = "bernoulli", n.trees = 100, interaction.depth = 3, shrinkage = 0.01)

gb_predictions <- predict(gb_model, newdata = test_data, type = "response")

gb_binary_predictions <- ifelse(gb_predictions > 0.5, 1, 0)

# ------------------------- FINAL ENSEMBLE METHOD MODEL ------------------------

final_predictions <- ifelse(svm_binary_predictions + rf_binary_predictions + gb_binary_predictions >= 2, 1, 0)


# STEP 4: EVALUATE MODEL --- For classification tasks, you can evaluate the model using metrics such as accuracy, precision, recall, or F1-score

# Calculate confusion matrix 
test_data$isWinner_binary <- as.numeric(test_data$isWinner) - 1

confusion_matrix <- confusionMatrix(data = as.factor(final_predictions), reference = as.factor(test_data$isWinner_binary))
confusion_matrix

# True Positive: Model correctly predicts a driver didn't win race (156)
# False Positive: Model incorrectly predicts driver didn't win, when they did (7)
# True Negative: Model correctly predicts a driver won the race (7)
# False Negative: Model incorrectly predicts a driver won race when they didn't (2)
accuracy <- confusion_matrix$overall["Accuracy"]

# PRECISION = 158 / (158+7) = 0.958
precision <- confusion_matrix$byClass["Pos Pred Value"]

# SENSITIVITY = 158 / (158 + 0) = 1
recall <- confusion_matrix$byClass["Sensitivity"]

# SPECIFICITY = (7 / [7 + 7]) = 0.5
specificity <- confusion_matrix$byClass["Specificity"]
  
# F1 SCORE= 2 * (0.958 * 1) / (0.958 + 1) = 0.978
f1score <- confusion_matrix$byClass["F1"]

summary <- data.frame(
  "SVM Prediction" = svm_binary_predictions,
  "RF Prediction" = rf_binary_predictions,
  "GB Prediction" = gb_binary_predictions,
  "Final Prediction" = final_predictions,
  "Actual Winner" = test_data$isWinner_binary
)

# STEP 5: VISUALIZE RESULTS
metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "Specificity", "F1 Score"),
  Value = c(accuracy, precision, recall, specificity, f1score)
)

ggplot(metrics_df, aes(x = Metric, y = Value, fill = Metric)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Value, 4)), vjust = -0.5) +
  labs(title = "Model Evaluation Metrics", y = "Value")


# STEP 6: USER INTERFACE
user_input <- data.frame(
  circuitId = rep(params$circuitId_isWinner, 20), # Replicate circuitId for all rows
  constructorId = c(9, 9, 6, 6, 1, 1, 131, 131, 117, 117, 215, 215, 214, 214, 210, 210, 15, 15, 3, 3),
  year = rep(params$year_isWinner, 20), # Replicate year for all rows
  driverId = c(830, 815, 844, 832, 846, 857, 1, 847, 4, 840, 852, 817, 842, 839, 825, 807, 822, 855, 848, 858), 
  grid = rep(0, 20), # Placeholder for grid
  milliseconds = rep(0, 20) # Placeholder for milliseconds
)
  
ui_svm_predictions <- predict(svm_model, newdata = user_input)
ui_svm_binary_predictions <- ifelse(ui_svm_predictions == TRUE, 1, 0)

ui_rf_predictions <- predict(rf_model, newdata = user_input, type = "response")
ui_rf_binary_predictions <- ifelse(ui_rf_predictions == TRUE, 1, 0)

ui_gb_predictions <- predict(gb_model, newdata = user_input, type = "response")
ui_gb_binary_predictions <- ifelse(ui_gb_predictions > 0.5, 1, 0)
    
ui_final_predictions <- ifelse(ui_svm_binary_predictions + ui_rf_binary_predictions + ui_gb_binary_predictions >= 2, 1, 0)

user_input <- user_input %>% 
  mutate(ui_final_predictions) %>% 
  left_join(drivers, by = "driverId")

winner_index <- which(user_input$ui_final_predictions == 1)
name <- user_input$forename[winner_index]
lastname <- user_input$surname[winner_index]

circuit <- unique(races$name[races$circuitId == params$circuitId_isWinner])

cat("Predicted Winner of the", params$year_isWinner, circuit, ":", name, lastname)
```

**Accuracy**: The overall accuracy of 0.9593 indicates the proportion of correct predictions made by the model on the entire test dataset.

**Precision:** The precision quantifies the model's ability to avoid incorrectly predicting a driver did not win the race, when they actually did (false positives). ***Therefore, a high precision score of 0.957 indicates that when the model predicts a driver as not winning the race, it is correct about 95.7% of the time.***

**Sensitivity (Recall):** The sensitivitiy quantifies the model's ability to successfully capture all cases where a driver didn't win (positive cases). The sensitivity score of 1 suggests that the model is able to successfully capture a high proportion of the cases where a driver actually did not win the race. ***This means that when a driver did not win the race, the model correctly identifies them as such 100% of the time.***

**Specificity:** The specificity quantifies the models ability to correctly identify cases where a driver won (negative cases). ***A specificity score of 0.5 means that approximately 50% of the time when a driver actually won the race, the model incorrectly predicts that they didn't win the race ***

**F1 Score:** The F1 score of 0.978 is a harmonic mean of precision and recall, providing a balanced measure of the model's performance. It combines both the precision and sensitivity of the model into a single metric.

**Balanced Accuracy:** The balanced accuracy of 0.75 accounts for class imbalance by taking the average of sensitivity and specificity. It provides a more reliable measure of model performance when dealing with imbalanced datasets.


# ______________________________________________________________________________

## 2. Linear Regression Model on XLK Stock Price

I successfully train a Machine Learning model by gathering and manipulating the relevant data, splitting it into two sets (train & test), training a linear regression model, and testing it on the data set aside for testing. The resulting model displays a Root Mean Square Error of only 0.69, meaning that on average, my predictions are off by $0.69. I also included a regression line to show that on average, the stock price has gone up in the past year as well as a perfect fit line in the other plot.

```{r, message=FALSE, warning=FALSE, fig.width=12, echo=FALSE}
# 1. Read the CSV files into data frames
data <- read_csv("XLK dataset.csv")

data$Date <- as.Date(data$Date, format("%m/%d/%Y"))
data$Date <- as.numeric(data$Date)

# 2. Split the dataset into training and testing subsets
training_data <- data[1:5884, ]
testing_data <- data[5885:6134, ]

# Train Model using linear regression model ('.' indicates all other variables will be used as predictors)
model <- lm(Close ~ ., data = training_data)

predictions <- predict(model, newdata = testing_data)

# Evaluate performance using Root Mean Square Error
rmse <- sqrt(mean((testing_data$Close - predictions)^2))
cat("Root Mean Squared Error: $", round(rmse, 2))
#summary(model) # We know the model is perfect when Median = 0 and absolute values of 1q = 3q

# Plot
ggplot(testing_data, aes(Date-19500, Close)) +
  geom_point(aes(col = "Actual Price")) +  # Testing Data points (Actual price values)
  geom_line(aes(y = predictions, col = "Predictions")) +  # Add a line for model predictions
  geom_smooth(method = "lm", aes(col = "Regression Line"), se = FALSE) + # Regression Line
  labs(x = "Time (Days in which Stock Market is Active)", y = "Stock Price", title = "XLK Stock Linear Regression Model Predictions", subtitle = "(In the last 365 Days)") +
  geom_text(aes(label = paste("Root Mean Square Error =", round(rmse, 4)), x = Inf, y = Inf), hjust = 1, vjust = 1, color = "black") + # RMSE Text
  scale_color_manual(name = "Legend", values = c("black", "blue", "yellow"))


  
plot(testing_data$Close, predictions, 
     xlab = "Observed", ylab = "Predicted", 
     main = "Observed vs. Predicted Values",
     col = "black")
# Red "Perfect" Line
abline(0, 1, col = "red")
```


# ______________________________________________________________________________

## 3. Linear Regression Model on **American House Prices** 

Recently, I found at our neighbors were selling their house and moving away. I was curious how much the house would cost so I went on Zillow and to my surprise, it was worth way more than ours. After looking at the pictures available, it was easy to understand why: they had a finished basement, a breakfast sunroom, and their house was nearly 1000 sqft bigger than ours. 

This got me wondering how important certain features have in a home, especially the number of beds, baths, and its size in sqft. I got to work building a linear regression model that is able to predict the price of a house given the features as input. All in all, it turned out pretty good.

```{r, fig.width=10, message=FALSE, echo=FALSE}
set.seed(1)

# Step 1: Extract Features
data <- read_csv("USA_Housing_Data.csv")

# Changing the name of the variables I will use that have spaces because R doesn't like spaces so it will allow for smoother data manipulation.
names(data)[names(data) == "Living Space"] <- "Living_Space"

# Assigning a unique numeric identifier for each state in the data to put in the lm() regression model (since it doesn't accept categorical variables when creating the formula)
data$State <- factor(data$State)
data$State_numeric <- as.integer(data$State)

  # 1A: Removing Missing Values
  clean_data <- na.omit(data)

  # 1B: Remove outliers
  z_scores <- scale(clean_data$Price)
  outliers <- which(abs(z_scores) > 0.5)

  outliers_beds <- which(clean_data$Beds > 12)

  outliers_baths <- which(clean_data$Baths > 12)

  outliers_space <- which(clean_data$Living_Space < 100 | clean_data$Living_Space > 9999)

  # 1C: Filtering the data so it's ready for modelling
  clean_data <- clean_data %>%
    filter(!Price %in% clean_data$Price[outliers]) %>%
    filter(!Beds %in% clean_data$Beds[outliers_beds]) %>%
    filter(!Baths %in% clean_data$Baths[outliers_baths]) %>%
    filter(!Living_Space %in% clean_data$Living_Space[outliers_space]) %>%
    select(Price, Beds, Baths, Living_Space, State, State_numeric)


  # Step 2: Separate into 2 datasets (Training & Testing)
split <- createDataPartition(clean_data$Price, p = 0.98, list = FALSE)

training_data <- clean_data[split, ]
testing_data <- clean_data[-split, ]
  

# Step 3: Train Model
model <- lm(Price ~., data = training_data)

predictions <- predict(model, newdata = testing_data)


# Step 4: Evaluate Model
rmse <- sqrt(mean((testing_data$Price - predictions)^2))

# Step 5: Display results, Price correlation with each important house feature.
ggplot(testing_data, aes(x = Living_Space, y = Price)) +
  geom_line(aes(y = predictions, color = "Predictions")) +
  geom_point(aes(color = "Actual Price")) +
  geom_smooth(method = "lm", aes(color = "Regression Line")) +
  labs(x = "Size (sqft)", y = "Price ($)", title = "Linear Regression Model Prediction for Price of Houses") +
  scale_y_continuous(labels = scales::comma, breaks = seq(500000, 3000000, by = 500000)) +
  scale_x_continuous(breaks = seq(1000, 6000, by = 1000)) +
  scale_color_manual(name = "Legend", values = c("black", "blue", "red"))

# ggplot(testing_data, aes(x = Beds, y = Price)) +
#   geom_point() +
#   geom_smooth(method = "lm") +
#   labs(x = "Number of Beds", y = "Price ($)", title = "Linear Regression Model Prediction for Price of Houses") +
#   scale_y_continuous(labels = scales::comma, breaks = seq(500000, 3000000, by = 500000)) +
#   scale_x_continuous(breaks = seq(1, 7, by = 1))
#
# ggplot(testing_data, aes(x = Baths, y = Price)) +
#   geom_point() +
#   geom_smooth(method = "lm") +
#   labs(x = "Number of Bathrooms", y = "Price ($)", title = "Linear Regression Model Prediction for Price of Houses") +
#   scale_y_continuous(labels = scales::comma, breaks = seq(500000, 3000000, by = 500000)) +
#   scale_x_continuous(breaks = seq(1, 6, by = 1))

# Create a mapping from state names to abbreviations
state_abbreviations <- setNames(state.abb, tolower(state.name))

# Convert state names in testing_data to abbreviations
testing_data$State_Abbreviation <- state_abbreviations[tolower(testing_data$State)]

ggplot(testing_data, aes(x = State_Abbreviation, y = Price)) +
  geom_boxplot() +
  labs(x = "State", y = "Price ($)", title = "Boxplot for Price of Houses by State") +
  scale_y_continuous(labels = scales::comma, breaks = seq(500000, 3000000, by = 500000))


plot(testing_data$Price, predictions,
     xlab = "Observed", ylab = "Predicted",
     main = "Observed vs. Predicted Values",
     col = "black")
# Red "Perfect" Line
abline(0, 1, col = "red")



# Step 6: Create Interface
# Creating a data frame with user input in param header of r markdown document
user_input <- data.frame( # If needed, use as.numeric() on numeric variables.
  Beds = params$num_beds,
  Baths = params$num_baths,
  Living_Space = params$living_space,
  State = params$state
)

# Check if state is in data
if (params$state %in% unique(clean_data$State)) {
  # Converting state input to its corresponding numeric identifier
  State_numeric <- clean_data$State_numeric[match(params$state, clean_data$State)]
  user_input$State_numeric <- State_numeric

  predicted_price <- predict(model, newdata = user_input)

  cat("Predicted price of a house in", params$state, "with", params$num_beds, "beds,", params$num_baths, "baths, and a living space of", params$living_space, "sqft: $", round(predicted_price, 2))
} else {
  stop("Unfortunately, we do not have a record of homes in your state, thus we cannot provide a prediction on the price of your house.")
}
```

# ______________________________________________________________________________

# Data Analysis Projects

## 1. 2011 Masters Golf Tournament Analysis

The plot below is a line graph I created visualizing the summary of the Masters 2011 Pro Golf Tournament, along with the performance of each golfer and the overall winner of the comptetition (Charl Schwartzel). 

```{r, fig.width=20, fig.height=12, warning=FALSE, message=FALSE, echo=FALSE}

load(url(
  "https://www.stat.osu.edu/~vqv/4194/data/masters2011-untidy.rda"
))

# Binds the rows of round1, round2,round3, and round4; specifies the name of the new column that will be created to store the source of each row in the resulting data frame, rounds
rounds <- bind_rows(round1, round2, round3, round4, .id = "round")


scorecard <- rounds %>% 
  pivot_longer(cols = "1":"18", names_to = "hole", values_to = "score") %>% 
  mutate(round = as.integer(round), hole = as.integer(hole), score = as.integer(score))


performance <- scorecard %>%
  left_join(course, by = "hole") %>%
  mutate(difference_to_par = score - par) %>%
  group_by(player) %>%
  mutate(cumulative_to_par = cumsum(difference_to_par)) %>%
  ungroup() %>%
  select(player, round, hole, difference_to_par, cumulative_to_par)

winner <- performance %>%
  filter(round == 4) %>%
  top_n(1, wt = -cumulative_to_par)
  
ggplot(performance) +
  geom_line(aes(x = hole, y = cumulative_to_par, col = player)) +
  facet_grid(. ~ round, labeller = labeller(round = c("1" = "round 1", "2" = "round 2", "3" = "round 3", "4" = "round 4"))) +
  geom_label(aes(x = hole - 4, y = cumulative_to_par, label = player), data = winner) +
  labs(title = "Performance Summary: Masters 2011 Pro Golf Tournament", x = "Hole", y = "Cumulative Score to Par", caption = "Overall winner: Charl Schwartzel") +
  theme_minimal() +
  theme(legend.position = "right", legend.key.size = unit(c(1, 1), "cm"), legend.text = element_text(size = 10))

```


# ______________________________________________________________________________

## 2. NFL 2023 QB Performance Analysis


### Overall QB Performance:

I'm a big NFL fan, and often times I see many people debating which QB performed the best in a season, or who deserves the MVP. As a result, I created a spreadsheet of all the NFL Quarterbacks that played a minimum of 100 snaps in the 2023-2024 season, compiling advanced statistics with the help of Pro Football Reference (https://www.pro-football-reference.com/). Using this data, I developed a formula in Excel to grade their efficiency, MVP rating, and overall performance, and then transferred the file into R to visualize my findings. The names displayed are my MVP Finalists according to my grading system, efficiency formula, and overall MVP Ratings.

(If you're interesting in which performance metrics I used or how I measured the overall Performance feel free to check out my dataset on my GitHub)

```{r, fig.width=12, warning=FALSE, message=FALSE, echo=FALSE}
qbdata <- read_csv("C:\\Users\\ajcon\\Downloads\\Portfolio\\qbdata.csv")

filtered_qbdata <- qbdata %>% 
  filter(TotSnaps > 100)

mvp_finalists <- filtered_qbdata %>% 
  top_n(5, filtered_qbdata$MVP_Rating)

ggplot(filtered_qbdata, aes(x = Efficiency, y = Grade, col = Player)) +
  geom_point() +
  geom_label_repel(data = mvp_finalists, aes(label = Player), nudge_x = -0.001, nudge_y = -0.05, segment.color = "black", segment.size = 0.5) +
  labs(x = "Weighted Efficiency Scale", y = "Grading Scale", title = "Overall QB Performance 2023-2024 NFL Season") +
  theme_minimal() +
  theme(legend.position = "right")

```

> As we can see, my MVP rating suggests that the 5 MVP Finalists be the named above. When comparing it to the real life finalists, only Jared Goff was not included (replaced by Christian McCaffrey).

> Note: In case you were wondering why the 2nd and 3rd highest Grade datapoints (Jake Browning & Kirk Cousins) were not considered an MVP candidate, it is because the formula that quantifies a player's mvp rating incorporates the player's total amount of snaps, and wins. Both of these players played less than half a season. 

## QB Performance by Team

```{r, fig.width=20, fig.height=15, echo=FALSE}
ggplot(filtered_qbdata, aes(x = Efficiency, y = Grade)) +
  geom_point() +
  geom_text(aes(label = Player), nudge_x = 0, nudge_y = -0.05) +
  facet_wrap(. ~ Team) +
  labs(x = "Efficiency", y = "Grade", title = "Quarterback Performance by Team")
```


# ______________________________________________________________________________

## 3. A Data-led Look into the History of the Olympics

 After gaining access to mulitple datasets of the Olympics containing every instance throughout every competition since the inaugural season back in 1896 (Greece) up until the 2016 Games in Brazil, I decided my free time would be well spent answering a couple of questions I, like many others (I think), have been wondering:

> 1. Does the economical stability of a country affect the number of athletes it sends to the olympics and the number of medals it wins?

> 2. Does hosting the olympics correlate to winning more medals that year?

### Part I

Below are the results I found for the first question, along with the code I wrote to filter and manipulate the data so I can visualize it in a more effective manner.

```{r, message=FALSE, echo=FALSE}
athlete_events <- read_csv(
   file = 'athlete_events.csv',
   col_types = cols(ID = 'i', Age = 'i', Height = 'i', Year = 'i')
)

# Creating a function to make the gapminer and olympics data compatible
nearest_year <- function(olympics_year) {
  gapminder_year <- seq(1952, 2007, by = 5)
  nearest_year <- gapminder_year[which.min(abs(olympics_year - gapminder_year))]
  return(nearest_year)
} 

olympics_data_medals_won <- athlete_events %>%
  filter(!is.na(Medal)) %>%
  count(Games, Event, NOC, Medal, Team, Year, Name) %>%
  mutate(year = nearest_year(Year))

olympics_data_athletes_sent <- athlete_events %>%
  count(Games, Event, NOC, Medal, Team, Year, Name) %>%
  mutate(year = nearest_year(Year))

country_money <- gapminder %>%
  filter(gdpPercap < 80000) %>% 
  group_by(country) %>%
  select(country, year, gdpPercap)

athletes_by_country_year <- olympics_data_athletes_sent %>%
 group_by(Team, Year) %>%
  summarise(Total_Athletes = n(), .groups = 'drop')

medals_by_country_year <- olympics_data_medals_won %>%
  group_by(Team, Year) %>% 
  summarise(Total_Medals = n(), .groups = 'drop')

joined_data_athletes <- inner_join(athletes_by_country_year, country_money, by = c("Year" = "year", "Team" = "country")) %>%
  filter(!is.na(gdpPercap))

joined_data_medals <- inner_join(medals_by_country_year, country_money, by = c("Year" = "year", "Team" = "country")) %>%
  filter(!is.na(gdpPercap))

ggplot(joined_data_athletes, aes(x = gdpPercap , y = Total_Athletes)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red", se = FALSE) + #Plotting the athlete correlation
  labs(title = "Number of Athletes vs Country's GDP Per Capita", x = "GDP Per Capita ($)", y = "Number of Athletes Country Sends")

ggplot(joined_data_medals, aes(x = gdpPercap , y = Total_Medals)) +
  geom_point() +
  geom_smooth(method = "lm", color = "blue", se = FALSE) + #lm creates a smooth line to show a clear representation
    labs(title = "Medals Won vs Country's GDP Per Capita", x = "GDP Per Capita ($)", y = "Medals Won")
```

> As we can see, there is in fact a positive correlation between a country’s gdp per capita and the number of medals and athletes a country has. This means that the higher the gdp is, the more medals it wins and more athletes it sends to the Olympics.


### Part II

```{r, include=FALSE}
# Importing relevant data sets
athlete_events  = read.csv("athlete_events.csv")
host_cities = read.csv("host_cities.csv")

# Renaming the 'city' variable in host_cities to 'City' so I can join the 2 datasets with key "City"
host_cities <- rename(host_cities, "City" = "city")

data <- full_join(athlete_events, host_cities, by = "City")
```

For the second question... I started by joining data sets together and creating a function that will filter the joint dataset for each country and in each of the seasons: determine whether they hosted or not. The function also displays a plot to compare the amount of medals that country won when they hosted vs when they did not. We will then compare and draw reasonable conclusions by creating a histogram containing the average number of medals all countries combined have won when they host vs in the competitions before.

```{r, echo=FALSE}
generate_country_medals_plot <- function(country_code, country_name, summer_hosts, winter_hosts) {
 
  # SUMMER
  summer_plot <- NULL
 
  if (length(summer_hosts) > 0) {
    summer_medals <- data %>%
      filter(NOC == country_code & !is.na(Medal) & Season == "Summer" & Year %in% c(1896:2016)) %>%
      distinct(Year, Event) %>%
      group_by(Year) %>%
      summarise(Medal_Count = n())
   
    summer_medals$Host <- ifelse(summer_medals$Year %in% summer_hosts, "Hosted", "Not Hosted")
   
    summer_plot <- ggplot(summer_medals, aes(x = Year, y = Medal_Count, fill = Host)) +
      geom_bar(stat = "identity", position = "dodge") +
      geom_text(aes(label = Year), vjust = -0.5, position = position_dodge(width = 0.9)) +
      labs(x = "Year", y = "Medals", title = paste("Summer Olympic Medals won by", country_name))
  }
 
  # WINTER
  winter_plot <- NULL
 
  if (length(winter_hosts) > 0) {
    winter_medals <- data %>%
      filter(NOC == country_code & !is.na(Medal) & Season == "Winter" & Year %in% c(1896:2016)) %>%
      distinct(Year, Event) %>%
      group_by(Year) %>%
      summarise(Medal_Count = n())
   
    winter_medals$Host <- ifelse(winter_medals$Year %in% winter_hosts, "Hosted", "Not Hosted")
   
    winter_plot <- ggplot(winter_medals, aes(x = Year, y = Medal_Count, fill = Host)) +
      geom_bar(stat = "identity", position = "dodge") +
      geom_text(aes(label = Year), position = position_dodge(width = 0.9)) +
      labs(x = "Year", y = "Medals", title = paste("Winter Olympic Medals won by", country_name))
  }
 
  list(summer_plot = summer_plot, winter_plot = winter_plot)
}
```


```{r, comment="", echo=FALSE, results='hide', fig.height=3, fig.width=5}
usa_plots <- generate_country_medals_plot("USA", "United States", c(1904, 1932, 1984, 1996), c(1932, 1960, 1980, 2002))
usa_plots

australia_plots <- generate_country_medals_plot("AUS", "Australia", c(1956, 2000), numeric(0))
australia_plots

austria_plots <- generate_country_medals_plot("AUT", "Austria", numeric(0), c(1964, 1976))
austria_plots

belgium_plots <- generate_country_medals_plot("BEL", "Belgium", 1920, numeric(0))
belgium_plots

brazil_plots <- generate_country_medals_plot("BRA", "Brazil", 2016, numeric(0))
brazil_plots

canada_plots <- generate_country_medals_plot("CAN", "Canada", 1976, c(1988, 2010))
canada_plots

china_plots <- generate_country_medals_plot("CHN", "China", 2008, numeric(0))
china_plots

finland_plots <- generate_country_medals_plot("FIN", "Finland", 1952, numeric(0))
finland_plots

france_plots <- generate_country_medals_plot("FRA", "France", c(1900, 1924), c(1924, 1968, 1992))
france_plots

germany_plots <- generate_country_medals_plot(c("GER", "GDR"), "Germany", c(1936, 1972), c(1936, 1972))
germany_plots

greece_plots <- generate_country_medals_plot("GRE", "Greece", c(1896, 2004), numeric(0))
greece_plots

italy_plots <- generate_country_medals_plot("ITA", "Italy", 1960, c(1956, 2006))
italy_plots

japan_plots <- generate_country_medals_plot("JPN", "Japan", 1964, c(1972, 1998))
japan_plots

mexico_plots <- generate_country_medals_plot("MEX", "Mexico", 1968, numeric(0))
mexico_plots

netherlands_plots <- generate_country_medals_plot("NED", "Netherlands", 1928, numeric(0))
netherlands_plots

norway_plots <- generate_country_medals_plot("NOR", "Norway", numeric(0), c(1952, 1994))
norway_plots

russia_plots <- generate_country_medals_plot(c("URS", "RUS"), "Russia/USSR", 1980, 2014)
russia_plots

skorea_plots <- generate_country_medals_plot("KOR", "South Korea", 1988, numeric(0))
skorea_plots

spain_plots <- generate_country_medals_plot("ESP", "Spain", 1992, numeric(0))
spain_plots

sweden_plots <- generate_country_medals_plot("SWE", "Sweden", c(1912, 1956), numeric(0))
sweden_plots

switzerland_plots <- generate_country_medals_plot("SUI", "Switzerland", numeric(0), c(1928, 1948))
switzerland_plots

uk_plots <- generate_country_medals_plot("GBR", "United Kingdom", c(1908, 1948, 2012), numeric(0))
uk_plots

yugoslavia_plots <- generate_country_medals_plot("YUG", "Yugoslavia", numeric(0), 1984)
yugoslavia_plots
```

As stated earlier, I created a histogram of the difference of medals **(by subtracting the medals won when they host minus the medals won in the olympic season directly prior)** to draw a reasonable conclusion.

```{r, echo=FALSE}
hist_data <- tibble(
  NOC = c("USA", "AUS", "AUT", "BEL", "BRA", "CAN", "CHN", "FIN", "FRA", "GER", "GRE", "ITA", "JPN", "MEX", "NED", "NOR", "RUS", "KOR", "ESP", "SWE", "SUI", "GBR", "YUG"),
  Medals_Won_Host = c(451, 83, 13, 33, 18, 39, 85, 22, 109, 102, 45, 47, 36, 9, 22, 29, 130, 30, 22, 64, 9, 165, 1),
  Medals_Won_Year_Before = c(265, 49, 8, 5, 17, 29, 55, 19, 52, 39, 16, 33, 22, 1, 11, 18, 84, 18, 4, 53, 5, 76, 3))

new <- hist_data %>%
  mutate(Distribution_difference = Medals_Won_Host - Medals_Won_Year_Before)

hist(new$Distribution_difference, xlab = "Difference between Medals (Host Season - Season Before)", main = "Histogram of the Distribution Difference", col = "lightgreen")
```

> We can see there is a positive host effect country on the amount of medals won when a country hosts the olympics vs when they don't because there is an overall positive difference.

## I hope my findings have been as interesting to you as they were to me !
